{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5221e738",
   "metadata": {},
   "source": [
    "# 설치 & 기본 설정 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d03f534",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (필요 시) 설치\n",
    "%pip -q install dpkt chardet\n",
    "\n",
    "# 경로 설정\n",
    "PCAP_PATH = \"../pcap_file/demo_pcap/demo_payload_preproc.pcap\"\n",
    "OUT_TXT   = \"samples_pre.txt\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b547df3",
   "metadata": {},
   "source": [
    "# HTTP 파싱/디코딩 유틸"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f237224",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dpkt, urllib.parse, base64, binascii, zlib, chardet, re, unicodedata\n",
    "from typing import Optional, Dict\n",
    "from urllib.parse import urlsplit, parse_qs\n",
    "from pathlib import Path\n",
    "\n",
    "def _maybe_decode_text(b: bytes) -> str:\n",
    "    if not b: return \"\"\n",
    "    guess = chardet.detect(b) or {}\n",
    "    enc = guess.get(\"encoding\") or \"utf-8\"\n",
    "    try:\n",
    "        return b.decode(enc, errors=\"ignore\")\n",
    "    except Exception:\n",
    "        return b.decode(\"utf-8\", errors=\"ignore\")\n",
    "\n",
    "def _http_extract(raw: bytes):\n",
    "    \"\"\"HTTP 요청/응답 파싱 → (mode, headers, body)\"\"\"\n",
    "    try:\n",
    "        if raw.startswith(b\"HTTP/1.\"):\n",
    "            resp = dpkt.http.Response(raw)\n",
    "            return (\"resp\", {k.lower(): v for k,v in resp.headers.items()}, resp.body or b\"\")\n",
    "        else:\n",
    "            req = dpkt.http.Request(raw)\n",
    "            return (\"req\", {k.lower(): v for k,v in req.headers.items()}, req.body or b\"\")\n",
    "    except (dpkt.dpkt.NeedData, dpkt.dpkt.UnpackError):\n",
    "        return (None, None, raw)\n",
    "\n",
    "def _decode_chunked(body: bytes) -> bytes:\n",
    "    out = bytearray(); i = 0\n",
    "    while True:\n",
    "        j = body.find(b\"\\r\\n\", i)\n",
    "        if j < 0: break\n",
    "        sz_hex = body[i:j].split(b\";\", 1)[0]\n",
    "        try: size = int(sz_hex, 16)\n",
    "        except ValueError: break\n",
    "        i = j + 2\n",
    "        if size == 0: break\n",
    "        out.extend(body[i:i+size]); i += size + 2\n",
    "    return bytes(out) if out else body\n",
    "\n",
    "def _maybe_gzip_deflate(headers: Optional[Dict[str,str]], body: bytes) -> bytes:\n",
    "    if not headers: return body\n",
    "    enc = headers.get(\"content-encoding\", \"\").lower()\n",
    "    if \"gzip\" in enc:\n",
    "        try: return zlib.decompress(body, 16 + zlib.MAX_WBITS)\n",
    "        except Exception: pass\n",
    "    if \"deflate\" in enc:\n",
    "        try: return zlib.decompress(body, -zlib.MAX_WBITS)\n",
    "        except Exception: pass\n",
    "    return body\n",
    "\n",
    "def _maybe_chunked(headers: Optional[Dict[str,str]], body: bytes) -> bytes:\n",
    "    if not headers: return body\n",
    "    if \"chunked\" in headers.get(\"transfer-encoding\",\"\").lower():\n",
    "        try: return _decode_chunked(body)\n",
    "        except Exception: return body\n",
    "    return body\n",
    "\n",
    "def headers_pick(h: Dict[str,str]) -> str:\n",
    "    keys = [\"content-type\",\"content-encoding\",\"transfer-encoding\"]\n",
    "    parts = [f\"h[{k[:2]}]={h[k]}\" for k in keys if h.get(k)]\n",
    "    return \" \".join(parts) if parts else \"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e437bf0d",
   "metadata": {},
   "source": [
    "# 전처리 규칙(소문자/NFKC/마스킹/토큰/길이)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6872398",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 정규식/룰\n",
    "RE_EMAIL = re.compile(r\"\\b[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[A-Za-z]{2,}\\b\")\n",
    "RE_IPV4  = re.compile(r\"\\b(?:\\d{1,3}\\.){3}\\d{1,3}\\b\")\n",
    "RE_HEX   = re.compile(r\"\\b[0-9A-Fa-f]{16,}\\b\")\n",
    "RE_B64   = re.compile(r\"\\b[A-Za-z0-9+/]{16,}={0,2}\\b\")\n",
    "RE_SID   = re.compile(r\"\\b[a-f0-9]{8,32}\\b\")\n",
    "RE_NUM   = re.compile(r\"\\d+\")\n",
    "RE_SPLIT = re.compile(r\"[^\\w/]+\")\n",
    "\n",
    "def normalize_nfkc_lower(s: str) -> str:\n",
    "    return unicodedata.normalize(\"NFKC\", s).lower()\n",
    "\n",
    "def url_decode_once(s: str) -> str:  # + → space 포함\n",
    "    return urllib.parse.unquote(s.replace(\"+\", \" \"))\n",
    "\n",
    "def mask_sensitive(s: str) -> str:\n",
    "    s = RE_EMAIL.sub(\"[EMAIL]\", s)\n",
    "    s = RE_IPV4.sub(\"[IP]\", s)\n",
    "    s = RE_HEX.sub(\"[HEX]\", s)\n",
    "    s = RE_B64.sub(\"[B64]\", s)\n",
    "    s = RE_SID.sub(\"[SID]\", s)\n",
    "    return s\n",
    "\n",
    "def tokenize_keep_slash_underscore(s: str):\n",
    "    parts = RE_SPLIT.split(s)\n",
    "    return [p for p in parts if p]\n",
    "\n",
    "def numbers_to_hash(tokens):\n",
    "    out = []\n",
    "    for t in tokens:\n",
    "        out.append(\"#NUM\" if RE_NUM.fullmatch(t) else RE_NUM.sub(\"#NUM\", t))\n",
    "    return out\n",
    "\n",
    "def head_tail_trim(tokens, max_len=320, head=192, tail=128):\n",
    "    if len(tokens) <= max_len: return tokens\n",
    "    return tokens[:head] + [\"[...]\"] + tokens[-tail:]\n",
    "\n",
    "def preprocess_text(raw: str):\n",
    "    s = normalize_nfkc_lower(raw)\n",
    "    s = url_decode_once(s)\n",
    "    s = mask_sensitive(s)\n",
    "    toks = tokenize_keep_slash_underscore(s)\n",
    "    toks = numbers_to_hash(toks)\n",
    "    return toks\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d3ecb1c",
   "metadata": {},
   "source": [
    "# 한 줄 표현 생성 + 실행(PCAP→samples_pre.txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a6568d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def body_signature_summary(body: bytes) -> str:\n",
    "    if not body: return \"BODY.len=0 BODY.sig=[-]\"\n",
    "    t = body[:64].decode(\"latin1\",\"ignore\").lower(); sig = []\n",
    "    if \"mz\" in t: sig.append(\"MZ\")\n",
    "    if \"pk\" in t: sig.append(\"PK\")\n",
    "    if \"gif89a\" in t: sig.append(\"GIF\")\n",
    "    if \"%pdf\" in t or \"pdf\" in t: sig.append(\"PDF\")\n",
    "    body256 = body[:256].decode(\"latin1\",\"ignore\")\n",
    "    if re.fullmatch(r\"[A-Za-z0-9+/=\\s]{32,}\", body256 or \"\"): sig.append(\"B64?\")\n",
    "    if re.fullmatch(r\"[0-9A-Fa-f\\s]{32,}\", body256 or \"\"):    sig.append(\"HEX?\")\n",
    "    return f\"BODY.len={len(body)} BODY.sig=[{','.join(sig) if sig else '-'}]\"\n",
    "\n",
    "def build_one_line_from_http(raw: bytes) -> str:\n",
    "    mode, hdrs, body = _http_extract(raw)\n",
    "    if mode == \"resp\":\n",
    "        return f\"RESP | {headers_pick(hdrs)} | {body_signature_summary(body)}\"\n",
    "    elif mode == \"req\":\n",
    "        req = dpkt.http.Request(raw)\n",
    "        host = hdrs.get(\"host\",\"-\"); ua = hdrs.get(\"user-agent\",\"-\")\n",
    "        u = urlsplit(req.uri); qkeys = \",\".join(sorted(parse_qs(u.query).keys())) if u.query else \"-\"\n",
    "        return f\"{req.method} {u.path or '/'} ? {qkeys} | host={host} | ua={ua} | {headers_pick(hdrs)} | {body_signature_summary(req.body or b'')}\"\n",
    "    # 파싱 실패 시\n",
    "    preview = raw[:120].decode(\"latin1\",\"ignore\").replace(\"\\r\",\" \").replace(\"\\n\",\" \")\n",
    "    return f\"RAW {preview}\"\n",
    "\n",
    "def add_sep_tokens(line: str) -> str:\n",
    "    parts = line.split(\"|\"); left = parts[0]; rest = [p.strip() for p in parts[1:]]\n",
    "    out = []\n",
    "    if \" ?\" in left:\n",
    "        path, q = left.split(\" ?\", 1)\n",
    "        out += [\"[SEP_PATH]\", path.strip(), \"[SEP_Q]\", q.strip()]\n",
    "    else:\n",
    "        out += [\"[SEP_PATH]\", left.strip()]\n",
    "    ua_blk   = next((p for p in rest if p.lower().startswith(\"ua=\")), \"\")\n",
    "    host_blk = next((p for p in rest if p.lower().startswith(\"host=\")), \"\")\n",
    "    hdr_blk  = next((p for p in rest if p.lower().startswith(\"h[\")), \"\")\n",
    "    body_blk = next((p for p in rest if p.lower().startswith(\"body.len=\")), \"\")\n",
    "    if host_blk: out += [\"[SEP_H]\", host_blk]\n",
    "    if ua_blk:   out += [\"[SEP_UA]\", ua_blk]\n",
    "    if hdr_blk:  out += [\"[SEP_H]\", hdr_blk]\n",
    "    if body_blk: out += [\"[SEP_BODY]\", body_blk]\n",
    "    return \" \".join(out)\n",
    "\n",
    "def iter_http_records(pcap_path: str):\n",
    "    with open(pcap_path, \"rb\") as f:\n",
    "        for ts, buf in dpkt.pcap.Reader(f):\n",
    "            eth = dpkt.ethernet.Ethernet(buf)\n",
    "            ip  = getattr(eth, \"data\", None)\n",
    "            if not isinstance(ip, dpkt.ip.IP):  continue\n",
    "            tcp = getattr(ip, \"data\", None)\n",
    "            if not isinstance(tcp, dpkt.tcp.TCP) or not tcp.data:  continue\n",
    "            if tcp.dport == 80 or tcp.sport == 80:\n",
    "                yield tcp.data\n",
    "\n",
    "def preprocess_one_line(raw_http: bytes, max_tokens=320) -> str:\n",
    "    line = build_one_line_from_http(raw_http)\n",
    "    line = add_sep_tokens(line)\n",
    "    toks = preprocess_text(line)\n",
    "    toks = head_tail_trim(toks, max_len=max_tokens, head=192, tail=128)\n",
    "    return \" \".join(toks)\n",
    "\n",
    "# 실행: PCAP → samples_pre.txt\n",
    "lines = []\n",
    "for raw in iter_http_records(PCAP_PATH):\n",
    "    s = preprocess_one_line(raw, max_tokens=320)\n",
    "    if s.strip(): lines.append(s)\n",
    "\n",
    "Path(OUT_TXT).write_text(\"\\n\".join(lines), encoding=\"utf-8\")\n",
    "print(f\"Wrote {len(lines)} lines -> {OUT_TXT}\")\n",
    "print(\"\\nPreview:\\n\", \"\\n\".join(lines[:5]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d50daeb",
   "metadata": {},
   "source": [
    "# 키워드 스캔 룰 & 스캐너"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad7b703b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from pathlib import Path\n",
    "\n",
    "IN_TXT  = \"samples_pre.txt\"\n",
    "OUT_KW  = \"suspect_keywords.txt\"\n",
    "OUT_HIT = \"hits_sample.txt\"\n",
    "\n",
    "# === 전처리된 한 줄 형식에 맞춘 관대한 패턴들 ===\n",
    "RULES = {\n",
    "    # 1) 실행/다운로드 명령 (하이픈/공백 변화 허용)\n",
    "    \"cmd\": r\"\\b(wget|curl|powershell\\s*-enc|mshta|certutil|bitsadmin|rundll32|python\\s*-c|regsvr32)\\b\",\n",
    "\n",
    "    # 2) 확장자: 전처리 때문에 '.'가 사라진 경우도 허용\n",
    "    \"ext\": r\"(\\.|\\\\b)(exe|dll|ps1|js|vbs|bat)\\\\b\",\n",
    "\n",
    "    # 3) 경로: 하이픈/점이 공백으로 바뀐 경우도 허용\n",
    "    \"path\": r\"(/wp[\\-\\s_]?includes/|/gate[.\\s_]?php|/api/.*/upload|wp-admin/admin[\\-\\s_]?ajax[.\\s_]?php)\",\n",
    "\n",
    "    # 4) 헤더: 하이픈이 공백으로 바뀐 MIME도 매치\n",
    "    \"header\": r\"(application[/\\-\\s]?x[/\\-\\s]?msdownload|application[/\\-\\s]?octet[/\\-\\s]?stream|multipart[/\\-\\s]?form[/\\-\\s]?data)\",\n",
    "\n",
    "    # 5) 인코딩 흔적: %25, %2f, .. /, +exec+, base64/eval/fromcharcode\n",
    "    \"encoding\": r\"(%25|%2f|\\.\\./|\\+exec\\+|base64|eval|fromcharcode)\"\n",
    "}\n",
    "\n",
    "# === 맥락 결합: 전처리된 토큰 형태를 반영 ===\n",
    "CONTEXT_RULES = [\n",
    "    # 헤더가 실행파일/바이너리 + 바디 시그니처(MZ/B64/HEX)\n",
    "    # h[ct] 대신 h co / h ct 모두 허용, body.sig=[..] → 'body sig mz|b64|hex' 형태 매치\n",
    "    (\n",
    "        r\"\\bh\\s*(\\[\\s*(ct|co)\\s*\\]|\\s*(ct|co))\\b.*?(application[/\\-\\s]?x[/\\-\\s]?msdownload|application[/\\-\\s]?octet[/\\-\\s]?stream)\",\n",
    "        r\"\\bbody\\s*len\\b.*\\bbody\\s*sig\\b.*\\b(mz|b64|hex)\\b\",\n",
    "        \"header+sig\"\n",
    "    ),\n",
    "    # 멀티파트 업로드 (파일명까지는 전처리에서 잘리기 쉬워서 헤더만 우선)\n",
    "    (\n",
    "        r\"multipart[/\\-\\s]?form[/\\-\\s]?data\",\n",
    "        r\"(/api/.*/upload|filename|body\\s*sig\\b)\",\n",
    "        \"multipart\"\n",
    "    ),\n",
    "    # C2 경로 + 인코딩/우회 흔적\n",
    "    (\n",
    "        r\"/gate[.\\s_]?php\",\n",
    "        r\"(powershell\\s*-enc|\\+exec\\+|%25|%2f|\\.\\./)\",\n",
    "        \"gate+enc\"\n",
    "    )\n",
    "]\n",
    "\n",
    "def scan_lines(lines):\n",
    "    hits = []\n",
    "    kw_set = set()\n",
    "\n",
    "    for line in lines:\n",
    "        L = line.lower()\n",
    "        matched = False\n",
    "\n",
    "        # 1) 단일 룰 스캔\n",
    "        for name, pat in RULES.items():\n",
    "            for m in re.finditer(pat, L, flags=re.I):\n",
    "                kw_set.add(m.group(0))\n",
    "                matched = True\n",
    "\n",
    "        # 2) 맥락 결합 스캔\n",
    "        for pat_a, pat_b, tag in CONTEXT_RULES:\n",
    "            if re.search(pat_a, L, flags=re.I) and re.search(pat_b, L, flags=re.I):\n",
    "                kw_set.add(f\"[CTX:{tag}]\")\n",
    "                matched = True\n",
    "\n",
    "        if matched:\n",
    "            hits.append(line)\n",
    "\n",
    "    return sorted(kw_set), hits\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce30127c",
   "metadata": {},
   "source": [
    "# 실행(스캔→파일 저장) + 미리보기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c70eb9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 실행\n",
    "lines = Path(IN_TXT).read_text(encoding=\"utf-8\").splitlines() if Path(IN_TXT).exists() else []\n",
    "kw, hits = scan_lines(lines)\n",
    "\n",
    "# 저장\n",
    "Path(OUT_KW).write_text(\"\\n\".join(kw), encoding=\"utf-8\")\n",
    "Path(OUT_HIT).write_text(\"\\n\".join(hits[:10]), encoding=\"utf-8\")\n",
    "\n",
    "print(f\"[OK] keywords -> {OUT_KW} ({len(kw)}개)\")\n",
    "print(f\"[OK] hits -> {OUT_HIT} (미리보기 {min(10, len(hits))}줄)\")\n",
    "\n",
    "# 디버그: 어떤 라인이 무엇 때문에 걸렸는지 보고 싶다면 아래 주석 해제\n",
    "# for h in hits[:10]:\n",
    "#     print(\"----\")\n",
    "#     print(h)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nis2025",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
