{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "6dfc69ea",
      "metadata": {},
      "source": [
        "# 셀 A) 전처리 유틸 (NFKC·소문자·마스킹·토큰화·길이제한)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "808f7b8d",
      "metadata": {},
      "outputs": [],
      "source": [
        "import re, unicodedata, urllib.parse, base64, binascii\n",
        "from typing import List, Dict\n",
        "\n",
        "RE_EMAIL = re.compile(r\"\\b[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[A-Za-z]{2,}\\b\")\n",
        "RE_IPV4  = re.compile(r\"\\b(?:\\d{1,3}\\.){3}\\d{1,3}\\b\")\n",
        "RE_HEX   = re.compile(r\"\\b[0-9A-Fa-f]{16,}\\b\")\n",
        "RE_B64   = re.compile(r\"\\b[A-Za-z0-9+/]{16,}={0,2}\\b\")\n",
        "RE_SID   = re.compile(r\"\\b[a-f0-9]{8,32}\\b\")  # 세션/토큰류(느슨)\n",
        "RE_NUM   = re.compile(r\"\\d+\")\n",
        "RE_SPLIT = re.compile(r\"[^\\w/\\.\\-]+\")\n",
        "\n",
        "# Keep these signature tokens as-is\n",
        "KEEP_ATOM = {\"b64\",\"hex\",\"mz\",\"pk\",\"gif\",\"pdf\"}\n",
        "\n",
        "def normalize_nfkc_lower(s: str) -> str:\n",
        "    return unicodedata.normalize(\"NFKC\", s).lower()\n",
        "\n",
        "def url_decode_once(s: str) -> str:\n",
        "    # + -> space 는 form-urlencoded에서 흔함\n",
        "    return urllib.parse.unquote(s.replace(\"+\", \" \"))\n",
        "\n",
        "def mask_sensitive(s: str) -> str:\n",
        "    s = RE_EMAIL.sub(\"[EMAIL]\", s)\n",
        "    s = RE_IPV4.sub(\"[IP]\", s)\n",
        "    # 긴 HEX/B64 먼저 치환(순서 중요)\n",
        "    s = RE_HEX.sub(\"[HEX]\", s)\n",
        "    s = RE_B64.sub(\"[B64]\", s)\n",
        "    # SID/토큰류\n",
        "    s = RE_SID.sub(\"[SID]\", s)\n",
        "    return s\n",
        "\n",
        "def tokenize_keep_slash_underscore(s: str) -> List[str]:\n",
        "    # '_' 는 단어문자(\\w)에 포함되어 유지됨, '/' 는 따로 허용\n",
        "    parts = RE_SPLIT.split(s)\n",
        "    return [p for p in parts if p]\n",
        "\n",
        "def numbers_to_hash(tokens: List[str]) -> List[str]:\n",
        "    out = []\n",
        "    for t in tokens:\n",
        "        tl = t.lower()\n",
        "        if tl in KEEP_ATOM:\n",
        "            out.append(t)\n",
        "            continue\n",
        "        if RE_NUM.fullmatch(t):\n",
        "            out.append(\"#NUM\")\n",
        "        else:\n",
        "            out.append(RE_NUM.sub(\"#NUM\", t))\n",
        "    return out\n",
        "def head_tail_trim(tokens: List[str], max_len=320, head=192, tail=128) -> List[str]:\n",
        "    if len(tokens) <= max_len:\n",
        "        return tokens\n",
        "    return tokens[:head] + [\"[...]\" ] + tokens[-tail:]\n",
        "\n",
        "def preprocess_text(raw: str) -> List[str]:\n",
        "    \"\"\"\n",
        "    전처리 전체: NFKC+lower -> URL 1회 디코드 -> 민감치환 -> 토큰화 -> 숫자군 치환\n",
        "    \"\"\"\n",
        "    s = normalize_nfkc_lower(raw)\n",
        "    s = url_decode_once(s)\n",
        "    s = mask_sensitive(s)\n",
        "    toks = tokenize_keep_slash_underscore(s)\n",
        "    toks = numbers_to_hash(toks)\n",
        "    return toks\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e5514bab",
      "metadata": {},
      "source": [
        "# 셀 B) HTTP 파트 추출 → 한 줄 표현 빌드\n",
        "\n",
        "- 구분 토큰: [SEP_PATH][SEP_Q][SEP_UA][SEP_H][SEP_BODY]\n",
        "- 바디는 길이/시그니처(B64/HEX/MZ/PK 등)만 요약"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "00ae6949",
      "metadata": {},
      "outputs": [],
      "source": [
        "import dpkt\n",
        "from urllib.parse import urlsplit, parse_qs\n",
        "\n",
        "SIG_HINTS = [\"mz\", \"pk\", \"gif89a\", \"%pdf\"]\n",
        "\n",
        "def body_signature_summary(body: bytes) -> str:\n",
        "    if not body:\n",
        "        return \"BODY.len=0\"\n",
        "    text = body[:64].decode(\"latin1\", errors=\"ignore\").lower()\n",
        "    sig = []\n",
        "    # 간단한 시그니처\n",
        "    if \"mz\" in text: sig.append(\"MZ\")\n",
        "    if \"pk\" in text: sig.append(\"PK\")\n",
        "    if \"gif89a\" in text: sig.append(\"GIF\")\n",
        "    if \"%pdf\" in text: sig.append(\"PDF\")\n",
        "    # 길고 규칙적인 텍스트면 B64/HEX 힌트\n",
        "    if re.fullmatch(r\"[A-Za-z0-9+/=\\s]{32,}\", body[:256].decode(\"latin1\", errors=\"ignore\") or \"\"):\n",
        "        sig.append(\"B64?\")\n",
        "    if re.fullmatch(r\"[0-9A-Fa-f\\s]{32,}\", body[:256].decode(\"latin1\", errors=\"ignore\") or \"\"):\n",
        "        sig.append(\"HEX?\")\n",
        "    return f\"BODY.len={len(body)} BODY.sig=[{','.join(sig) if sig else '-'}]\"\n",
        "\n",
        "def headers_pick(d: Dict[str,str]) -> str:\n",
        "    # 관심 헤더만 축약\n",
        "    keys = [\"content-type\",\"content-encoding\",\"transfer-encoding\"]\n",
        "    parts = []\n",
        "    for k in keys:\n",
        "        v = d.get(k)\n",
        "        if v:\n",
        "            parts.append(f\"h[{k[:2]}]={v}\")\n",
        "    return \" \".join(parts) if parts else \"\"\n",
        "\n",
        "def build_one_line_from_http(raw: bytes) -> str:\n",
        "    \"\"\"\n",
        "    METHOD PATH ? query_keys | HOST=... | UA=... | H[...] | BODY.len=... BODY.sig=...\n",
        "    \"\"\"\n",
        "    line = \"\"\n",
        "    try:\n",
        "        if raw.startswith(b\"HTTP/1.\"):\n",
        "            # Response\n",
        "            resp = dpkt.http.Response(raw)\n",
        "            hdrs = {k.lower(): v for k,v in resp.headers.items()}\n",
        "            body = resp.body or b\"\"\n",
        "            hsum = headers_pick(hdrs)\n",
        "            line = f\"RESP {resp.status} {resp.reason} | {hsum} | {body_signature_summary(body)}\"\n",
        "        else:\n",
        "            # Request\n",
        "            req = dpkt.http.Request(raw)\n",
        "            hdrs = {k.lower(): v for k,v in req.headers.items()}\n",
        "            host = hdrs.get(\"host\",\"-\")\n",
        "            ua   = hdrs.get(\"user-agent\",\"-\")\n",
        "            # PATH / QUERY keys\n",
        "            u = urlsplit(req.uri)\n",
        "            qkeys = \",\".join(sorted(parse_qs(u.query).keys())) if u.query else \"-\"\n",
        "            hsum  = headers_pick(hdrs)\n",
        "            body  = req.body or b\"\"\n",
        "            base  = f\"{req.method} {u.path or '/'} ? {qkeys} | host={host} | ua={ua} | {hsum} | {body_signature_summary(body)}\"\n",
        "            line  = base\n",
        "    except (dpkt.dpkt.NeedData, dpkt.dpkt.UnpackError):\n",
        "        # HTTP 파싱 실패 시 원문 앞부분만\n",
        "        preview = raw[:120].decode(\"latin1\",\"ignore\").replace(\"\\r\",\" \").replace(\"\\n\",\" \")\n",
        "        line = f\"RAW {preview}\"\n",
        "    return line\n",
        "\n",
        "def add_sep_tokens(line: str) -> str:\n",
        "    \"\"\"\n",
        "    구분 토큰 삽입: PATH, QUERY, UA, HEADERS, BODY 순서\n",
        "    \"\"\"\n",
        "    # 예상 포맷을 기준으로 간단 분해\n",
        "    # ... PATH ? Q | host=... | ua=... | h[..]=.. | BODY...\n",
        "    parts = line.split(\"|\")\n",
        "    left = parts[0]\n",
        "    rest = [p.strip() for p in parts[1:]]\n",
        "    out = []\n",
        "\n",
        "    # PATH / Q\n",
        "    if \" ?\" in left:\n",
        "        path, q = left.split(\" ?\", 1)\n",
        "        out += [\"[SEP_PATH]\", path.strip(), \"[SEP_Q]\", q.strip()]\n",
        "    else:\n",
        "        out += [\"[SEP_PATH]\", left.strip()]\n",
        "\n",
        "    # UA / HOST / HEADERS / BODY 위치 조정\n",
        "    ua_blk = next((p for p in rest if p.lower().startswith(\"ua=\")), \"\")\n",
        "    host_blk = next((p for p in rest if p.lower().startswith(\"host=\")), \"\")\n",
        "    hdr_blk = next((p for p in rest if p.lower().startswith(\"h[\")), \"\")\n",
        "    body_blk = next((p for p in rest if p.lower().startswith(\"body.len=\")), \"\")\n",
        "\n",
        "    if host_blk: out += [\"[SEP_H]\", host_blk]\n",
        "    if ua_blk:   out += [\"[SEP_UA]\", ua_blk]\n",
        "    if hdr_blk:  out += [\"[SEP_H]\", hdr_blk]\n",
        "    if body_blk: out += [\"[SEP_BODY]\", body_blk]\n",
        "\n",
        "    return \" \".join(out)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "12a63175",
      "metadata": {},
      "source": [
        "# 셀 C) 전처리 파이프라인 → 토큰 제한 → 한 줄 저장 (samples_pre.txt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1c42d7ef",
      "metadata": {},
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "\n",
        "PCAP_PATH = \"../pcap_file/demo_pcap/demo_payload_preproc.pcap\"\n",
        "OUT_TXT   = \"samples_pre.txt\"\n",
        "\n",
        "def iter_http_records(pcap_path: str):\n",
        "    with open(pcap_path, \"rb\") as f:\n",
        "        for ts, buf in dpkt.pcap.Reader(f):\n",
        "            eth = dpkt.ethernet.Ethernet(buf)\n",
        "            ip = getattr(eth, \"data\", None)\n",
        "            if not isinstance(ip, dpkt.ip.IP): \n",
        "                continue\n",
        "            tcp = getattr(ip, \"data\", None)\n",
        "            if not isinstance(tcp, dpkt.tcp.TCP) or not tcp.data:\n",
        "                continue\n",
        "            if tcp.dport == 80 or tcp.sport == 80:\n",
        "                yield tcp.data\n",
        "\n",
        "def preprocess_one_line(raw_http_bytes: bytes, max_tokens=320) -> str:\n",
        "    # 1) HTTP 요약 한 줄 생성\n",
        "    base_line = build_one_line_from_http(raw_http_bytes)\n",
        "    # 2) SEP 토큰 삽입\n",
        "    with_sep  = add_sep_tokens(base_line)\n",
        "    # 3) 전처리(소문자/NFKC/URL 1회/마스킹/토큰화/숫자치환)\n",
        "    toks = preprocess_text(with_sep)\n",
        "    # 4) 길이 제한(head-tail 우선)\n",
        "    toks = head_tail_trim(toks, max_len=max_tokens, head=192, tail=128)\n",
        "    # 5) 최종 한 줄\n",
        "    return \" \".join(toks)\n",
        "\n",
        "# 실행: pcap → samples_pre.txt\n",
        "lines = []\n",
        "for raw in iter_http_records(PCAP_PATH):\n",
        "    line = preprocess_one_line(raw, max_tokens=320)\n",
        "    if line.strip():\n",
        "        lines.append(line)\n",
        "\n",
        "Path(OUT_TXT).write_text(\"\\n\".join(lines), encoding=\"utf-8\")\n",
        "print(f\"Wrote {len(lines)} lines -> {OUT_TXT}\")\n",
        "print(\"\\nPreview:\\n\", \"\\n\".join(lines[:3]))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "60108594",
      "metadata": {},
      "source": [
        "# 셀 D) (선택) 결과 테이블 미리보기 & 길이 점검"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ba68dc71",
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.DataFrame({\n",
        "    \"line\": lines,\n",
        "    \"len_tokens\": [len(l.split()) for l in lines]\n",
        "})\n",
        "df\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "nis2025",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
