{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "40430f06",
   "metadata": {},
   "source": [
    "# 설치 & 경로/설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83393e90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# (필요 시) 설치\n",
    "%pip -q install transformers pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ce1ab5b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 파일 경로\n",
    "IN_TXT  = \"../day1/samples_pre.txt\"\n",
    "OUT_CSV = \"./bert_inputs.csv\"\n",
    "OUT_META= \"./tokenizer_meta.json\"\n",
    "OUT_STAT= \"./stats.txt\"\n",
    "\n",
    "# 토크나이저/길이 설정\n",
    "MODEL_NAME = \"bert-base-uncased\"   # 필요시 distilbert-base-uncased 등\n",
    "MAX_LEN    = 320                   # 권장: 256~384 범위\n",
    "PAD        = \"max_length\"\n",
    "TRUNC      = \"longest_first\"\n",
    "ADD_SPE    = True                  # [CLS]/[SEP] 자동"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9c9835f",
   "metadata": {},
   "source": [
    "## bert-base-uncased\n",
    "- bert\n",
    "\t- 모델 구조가 BERT (Bidirectional Encoder Representations from Transformers) 임을 의미.\n",
    "\t- 구글이 2018년에 발표한 대표적인 Transformer 기반 언어모델.\n",
    "\t- 특징: 문장을 양방향으로 동시에 보면서 문맥을 이해하는 능력.\n",
    "\n",
    "- base\n",
    "\t- 모델의 크기(scale).\n",
    "\t- bert-base: 12 layers (transformer blocks), hidden size=768, attention heads=12, 파라미터 약 110M.\n",
    "\t- bert-large: 24 layers, hidden size=1024, attention heads=16, 파라미터 약 340M.\n",
    "\n",
    "- uncased\n",
    "\t- \"대소문자 구분을 하지 않는다\"는 의미.\n",
    "\t- 토큰화할 때 모든 단어를 소문자로 변환하고, 사전도 소문자 기반으로 구성됨."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d884bc8",
   "metadata": {},
   "source": [
    "### bert-base-uncased 사양 요약\n",
    "- Layers: 12 (Transformer Encoder blocks)\n",
    "- Hidden size: 768\n",
    "- Attention heads: 12\n",
    "- Parameters: 약 110M\n",
    "- Vocabulary size: 30,522 (WordPiece 토크나이저)\n",
    "- 학습 데이터: BookCorpus (8억 단어) + Wikipedia (25억 단어)\n",
    "- 특수 토큰: [CLS], [SEP], [PAD], [MASK], [UNK] 포함"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87491cc9",
   "metadata": {},
   "source": [
    "# 2. 입력 로드(빈 줄 제거) + 간단 통계"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "768c91bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "raw_lines:  ['sep_path get /wp B#NUM php sep_q file sep_h host example com sep_ua ua mozilla/#NUM #NUM charon inferno sep_body body len #NUM body sig', 'sep_path resp sep_h h co application/x msdownload h co gzip h tr chunked sep_body body len #NUM body sig mz']\n",
      "texts:  ['sep_path get /wp B#NUM php sep_q file sep_h host example com sep_ua ua mozilla/#NUM #NUM charon inferno sep_body body len #NUM body sig', 'sep_path resp sep_h h co application/x msdownload h co gzip h tr chunked sep_body body len #NUM body sig mz']\n",
      "Loaded 2 lines from ../day1/samples_pre.txt\n",
      "Tokens per line (space-split) -> mean=21.5, median=22, max=23\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import numpy as np\n",
    "\n",
    "# 입력 읽기\n",
    "raw_lines = Path(IN_TXT).read_text(encoding=\"utf-8\").splitlines()\n",
    "print(\"raw_lines: \", raw_lines)\n",
    "texts = [ln.strip() for ln in raw_lines if ln.strip()]\n",
    "print(\"texts: \", texts)\n",
    "print(f\"Loaded {len(texts)} lines from {IN_TXT}\")\n",
    "\n",
    "# 토큰 수(스페이스 분할 기준) 대략 통계\n",
    "tok_counts = [len(t.split()) for t in texts]\n",
    "if tok_counts:\n",
    "    a = np.array(tok_counts)\n",
    "    print(f\"Tokens per line (space-split) -> mean={a.mean():.1f}, median={np.median(a):.0f}, max={a.max()}\")\n",
    "else:\n",
    "    print(\"WARNING: no texts loaded.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aae3d754",
   "metadata": {},
   "source": [
    "# Tokenizer 로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a10d0040-1c4b-4b11-9a68-23bf866771eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: huggingface_hub==0.34.4 in d:\\anaconda3\\envs\\iesc2025\\lib\\site-packages (0.34.4)\n",
      "Requirement already satisfied: filelock in c:\\users\\손진혁\\appdata\\roaming\\python\\python312\\site-packages (from huggingface_hub==0.34.4) (3.15.4)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in d:\\anaconda3\\envs\\iesc2025\\lib\\site-packages (from huggingface_hub==0.34.4) (2025.7.0)\n",
      "Requirement already satisfied: packaging>=20.9 in d:\\anaconda3\\envs\\iesc2025\\lib\\site-packages (from huggingface_hub==0.34.4) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in d:\\anaconda3\\envs\\iesc2025\\lib\\site-packages (from huggingface_hub==0.34.4) (6.0.2)\n",
      "Requirement already satisfied: requests in d:\\anaconda3\\envs\\iesc2025\\lib\\site-packages (from huggingface_hub==0.34.4) (2.32.4)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in d:\\anaconda3\\envs\\iesc2025\\lib\\site-packages (from huggingface_hub==0.34.4) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in d:\\anaconda3\\envs\\iesc2025\\lib\\site-packages (from huggingface_hub==0.34.4) (4.12.2)\n",
      "Requirement already satisfied: colorama in d:\\anaconda3\\envs\\iesc2025\\lib\\site-packages (from tqdm>=4.42.1->huggingface_hub==0.34.4) (0.4.6)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in d:\\anaconda3\\envs\\iesc2025\\lib\\site-packages (from requests->huggingface_hub==0.34.4) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in d:\\anaconda3\\envs\\iesc2025\\lib\\site-packages (from requests->huggingface_hub==0.34.4) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in d:\\anaconda3\\envs\\iesc2025\\lib\\site-packages (from requests->huggingface_hub==0.34.4) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in d:\\anaconda3\\envs\\iesc2025\\lib\\site-packages (from requests->huggingface_hub==0.34.4) (2025.8.3)\n"
     ]
    }
   ],
   "source": [
    "%pip unistall huggingface_hub \n",
    "%pip install huggingface_hub==0.34.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f9b719dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'model_name_or_path': 'bert-base-uncased',\n",
       " 'vocab_size': 30522,\n",
       " 'max_length': 320,\n",
       " 'padding': 'max_length',\n",
       " 'truncation': 'longest_first',\n",
       " 'add_special_tokens': True,\n",
       " 'do_lower_case': True,\n",
       " 'special_tokens_map': {'unk_token': '[UNK]',\n",
       "  'sep_token': '[SEP]',\n",
       "  'pad_token': '[PAD]',\n",
       "  'cls_token': '[CLS]',\n",
       "  'mask_token': '[MASK]'}}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "meta = {\n",
    "    \"model_name_or_path\": MODEL_NAME,\n",
    "    \"vocab_size\": tokenizer.vocab_size,\n",
    "    \"max_length\": MAX_LEN,\n",
    "    \"padding\": PAD,\n",
    "    \"truncation\": TRUNC,\n",
    "    \"add_special_tokens\": ADD_SPE,\n",
    "    \"do_lower_case\": getattr(tokenizer, \"do_lower_case\", None),\n",
    "    \"special_tokens_map\": tokenizer.special_tokens_map\n",
    "}\n",
    "meta\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54b2e959",
   "metadata": {},
   "source": [
    "# 배치 토크나이징 → input_ids / attention_mask 생성 + 스팟체크"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b4ed3b33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapes -> input_ids: 2×320, attention_mask: 2×320\n",
      "[101, 19802, 1035, 4130, 2131, 1013, 1059, 2361, 1038, 1001, 16371, 2213, 25718, 19802, 1035, 1053, 5371, 19802, 1035, 1044, 3677, 2742, 4012, 19802, 1035, 25423, 25423, 9587, 5831, 4571, 1013, 1001, 16371, 2213, 1001, 16371, 2213, 25869, 2239, 21848, 19802, 1035, 2303, 2303, 18798, 1001, 16371, 2213, 2303, 9033, 2290, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "decode[0]: sep _ path get / wp b # num php sep _ q file sep _ h host example com sep _ ua ua mozilla / # num # num charon inferno sep _ body body len # num body sig\n",
      "mask_sum[0]: 52\n"
     ]
    }
   ],
   "source": [
    "enc = tokenizer(\n",
    "    texts,\n",
    "    add_special_tokens=ADD_SPE,\n",
    "    return_attention_mask=True,\n",
    "    padding=PAD,\n",
    "    truncation=True,                 # longest_first는 pair에서 의미; 단일은 True로 충분\n",
    "    max_length=MAX_LEN\n",
    ")\n",
    "input_ids = enc[\"input_ids\"]            # List[List[int]] (N×L)\n",
    "attention_mask = enc[\"attention_mask\"]  # List[List[int]] (N×L)\n",
    "\n",
    "N = len(texts)\n",
    "L = len(input_ids[0]) if N else 0\n",
    "print(f\"Shapes -> input_ids: {len(input_ids)}×{L}, attention_mask: {len(attention_mask)}×{L}\")\n",
    "print(input_ids[0])\n",
    "# 스팟체크: 0번째 문장 복원/마스크 합계\n",
    "if N:\n",
    "    print(\"decode[0]:\", tokenizer.decode(input_ids[0], skip_special_tokens=True)[:200])\n",
    "    print(\"mask_sum[0]:\", sum(attention_mask[0]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "368c1233",
   "metadata": {},
   "source": [
    "# CSV/메타/통계 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "06f12cd6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>input_ids</th>\n",
       "      <th>attention_mask</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>sep_path get /wp B#NUM php sep_q file sep_h ho...</td>\n",
       "      <td>101 19802 1035 4130 2131 1013 1059 2361 1038 1...</td>\n",
       "      <td>1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>sep_path resp sep_h h co application/x msdownl...</td>\n",
       "      <td>101 19802 1035 4130 24501 2361 19802 1035 1044...</td>\n",
       "      <td>1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  \\\n",
       "0  sep_path get /wp B#NUM php sep_q file sep_h ho...   \n",
       "1  sep_path resp sep_h h co application/x msdownl...   \n",
       "\n",
       "                                           input_ids  \\\n",
       "0  101 19802 1035 4130 2131 1013 1059 2361 1038 1...   \n",
       "1  101 19802 1035 4130 24501 2361 19802 1035 1044...   \n",
       "\n",
       "                                      attention_mask  \n",
       "0  1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 ...  \n",
       "1  1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 ...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved CSV -> ./bert_inputs.csv (rows=2)\n",
      "Saved meta -> ./tokenizer_meta.json\n",
      "Saved stats -> ./stats.txt\n"
     ]
    }
   ],
   "source": [
    "import json, pandas as pd\n",
    "\n",
    "def arr_to_str(arr):\n",
    "    return \" \".join(str(x) for x in arr)\n",
    "\n",
    "rows = []\n",
    "for t, ids, m in zip(texts, input_ids, attention_mask):\n",
    "    rows.append({\n",
    "        \"text\": t,\n",
    "        \"input_ids\": arr_to_str(ids),\n",
    "        \"attention_mask\": arr_to_str(m),\n",
    "    })\n",
    "\n",
    "df = pd.DataFrame(rows, columns=[\"text\",\"input_ids\",\"attention_mask\"])\n",
    "display(df)\n",
    "df.to_csv(OUT_CSV, index=False, encoding=\"utf-8\")\n",
    "print(f\"Saved CSV -> {OUT_CSV} (rows={len(df)})\")\n",
    "\n",
    "with open(OUT_META, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(meta, f, ensure_ascii=False, indent=2)\n",
    "print(f\"Saved meta -> {OUT_META}\")\n",
    "\n",
    "# 간단 통계\n",
    "mask_sums = [sum(m) for m in attention_mask] if attention_mask else []\n",
    "with open(OUT_STAT, \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(f\"lines={len(texts)}\\n\")\n",
    "    if mask_sums:\n",
    "        f.write(f\"attn_sum_mean={float(np.mean(mask_sums)):.1f}\\n\")\n",
    "        f.write(f\"attn_sum_median={float(np.median(mask_sums)):.1f}\\n\")\n",
    "        f.write(f\"attn_sum_max={int(np.max(mask_sums))}\\n\")\n",
    "print(f\"Saved stats -> {OUT_STAT}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef149cfb",
   "metadata": {},
   "source": [
    "### 커스텀 토크나이저\n",
    "- 기존 BERT 토크나이저에 토큰만 추가\n",
    "``` python\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "MODEL_NAME = \"bert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "# 1) 추가할 토큰 정의\n",
    "new_tokens = [\"[URL]\", \"[IP]\", \"cmd.exe\", \"powershell\"]  # 예시\n",
    "num_added = tokenizer.add_tokens(new_tokens, special_tokens=False)\n",
    "\n",
    "# 2) 모델 로드 + 임베딩 크기 리사이즈\n",
    "model = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME, num_labels=13)\n",
    "if num_added > 0:\n",
    "    model.resize_token_embeddings(len(tokenizer))  # 새 토큰 임베딩은 무작위 초기화\n",
    "\n",
    "# (옵션) 저장\n",
    "tokenizer.save_pretrained(\"./ckpt_tok_custom\")\n",
    "model.save_pretrained(\"./ckpt_tok_custom\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "313cea03-43b5-49d4-a792-536d2f3d9b02",
   "metadata": {},
   "source": [
    "### 커스텀 토크나이저\n",
    "- 기존 BERT 토크나이저에 토큰만 추가\n",
    "``` python\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "MODEL_NAME = \"bert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "# 1) 추가할 토큰 정의\n",
    "new_tokens = [\"[URL]\", \"[IP]\", \"cmd.exe\", \"powershell\"]  # 예시\n",
    "num_added = tokenizer.add_tokens(new_tokens, special_tokens=False)\n",
    "\n",
    "# 2) 모델 로드 + 임베딩 크기 리사이즈\n",
    "model = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME, num_labels=13)\n",
    "if num_added > 0:\n",
    "    model.resize_token_embeddings(len(tokenizer))  # 새 토큰 임베딩은 무작위 초기화\n",
    "\n",
    "# (옵션) 저장\n",
    "tokenizer.save_pretrained(\"./ckpt_tok_custom\")\n",
    "model.save_pretrained(\"./ckpt_tok_custom\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8905680e",
   "metadata": {},
   "source": [
    "- 커스텀 토크나이저\n",
    "\t1. 기존 bert-base-uncased 가중치를 부분 로드(임베딩만 새로 초기화)\n",
    "\t\t- 장점: 나머지 트랜스포머 블록 가중치를 최대한 재사용\n",
    "\t\t- 주의: 토큰화가 달라져도 BERT 내부는 그대로라 초기 성능이 다소 흔들릴 수 있음\n",
    "\t\t- 토크나이저 학습 (예: tokenizers 또는 sentencepiece)\n",
    "\t\t- 학습된 vocab/merges로 BertTokenizerFast 생성\n",
    "\t\t- 모델은 bert-base-uncased에서 불러오되, 임베딩 사이즈 불일치는 무시하고 새로 리사이즈\n",
    "```python\n",
    "# 2-A-1) (예시) tokenizers로 WordPiece/BPE 학습\n",
    "from tokenizers import Tokenizer, models, trainers, pre_tokenizers, processors\n",
    "from pathlib import Path\n",
    "\n",
    "files = [\"corpus1.txt\", \"corpus2.txt\"]  # 도메인 코퍼스\n",
    "tokenizer_tr = Tokenizer(models.WordPiece(unk_token=\"[UNK]\"))\n",
    "tokenizer_tr.pre_tokenizer = pre_tokenizers.Whitespace()\n",
    "trainer = trainers.WordPieceTrainer(vocab_size=32000, special_tokens=[\"[PAD]\",\"[UNK]\",\"[CLS]\",\"[SEP]\",\"[MASK]\"])\n",
    "tokenizer_tr.train(files, trainer)\n",
    "Path(\"my_vocab.json\").write_text(tokenizer_tr.to_str(), encoding=\"utf-8\")  # 또는 tokenizer_tr.save(\"tokenizer.json\")\n",
    "\n",
    "# 2-A-2) HF 토크나이저로 래핑\n",
    "from transformers import BertTokenizerFast, AutoModel\n",
    "# WordPiece라면 vocab.txt 형태가 더 흔합니다 (tokenizers JSON → vocab.txt 변환 필요)\n",
    "# 여기서는 vocab.txt가 준비돼 있다고 가정\n",
    "tok = BertTokenizerFast(vocab_file=\"vocab.txt\", do_lower_case=True)\n",
    "\n",
    "# 2-A-3) 모델 로드 (임베딩 불일치 허용)\n",
    "model = AutoModel.from_pretrained(\n",
    "    \"bert-base-uncased\",\n",
    "    ignore_mismatched_sizes=True\n",
    ")\n",
    "\n",
    "# 2-A-4) 임베딩 리사이즈\n",
    "model.resize_token_embeddings(len(tok))\n",
    "\n",
    "# 저장\n",
    "tok.save_pretrained(\"./ckpt_tok_custom_full\")\n",
    "model.save_pretrained(\"./ckpt_tok_custom_full\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99545ddd",
   "metadata": {},
   "source": [
    "\t2. 새 config로 BERT를 새로 초기화\n",
    "\t\t- 장점: 완전한 자유도 (vocab 크기/스페셜 토큰/최대 길이 등)\n",
    "\t\t- 단점: 사전학습이 없으면 성능이 매우 낮음 → 대규모 프리트레이닝 필요\n",
    "```python\n",
    "from transformers import BertConfig, BertForMaskedLM, BertTokenizerFast\n",
    "\n",
    "tok = BertTokenizerFast(vocab_file=\"vocab.txt\", do_lower_case=True)\n",
    "\n",
    "config = BertConfig(\n",
    "    vocab_size=len(tok),\n",
    "    hidden_size=768,\n",
    "    num_hidden_layers=12,\n",
    "    num_attention_heads=12,\n",
    "    intermediate_size=3072,\n",
    "    max_position_embeddings=512\n",
    ")\n",
    "model = BertForMaskedLM(config)  # 랜덤 초기화\n",
    "\n",
    "# → 대규모 MLM 프리트레이닝이 필요\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
